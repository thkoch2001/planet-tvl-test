<html>
  <head>
    <title>Planet TVL</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="generator" content="planet-mars">
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="planet.css" type="text/css">
    <link rel="alternate" type="application/xml+atom" title="Planet Haskell Atom Feed" href="atom.xml">
  </head>
  <body>
    <header>
      <h1>Planet TVL</h1>
    </header>
    <div id="maincontainer">
      <main>
        <article>
            <h2 class="entry_header">
              <a href="https://tech.j4m3s.eu/posts/minimal-jre-nixos/">
                7x JVM Reduction: Nix Your Way to Lighter Docker Images
                </a>
            </h2>
            <div class="entry_meta">
              <date>
                <span>11.01.2025 11:14</span>
                </date>
              </div>

            <div class="entry_summary">
                Reducing by 7x the size of the JVM for fun and profit
              </div>
            </article>
        <hr class="entry_sep">
          <article>
            <h2 class="entry_header">
              <a href="https://blog.benjojo.co.uk/post/sfp-experiment-ultra-long-range-toslink">
                Building Ultra Long Range TOSLINK
                </a>
            </h2>
            <div class="entry_meta">
              <date>
                <span>07.01.2025 12:24</span>
                </date>
              &mdash; <span class="entry_author">author</span>
              </div>

            <div class="entry_summary">
                <h1>Building Ultra Long Range TOSLINK</h1>

<p><img src="https://blog.benjojo.co.uk/asset/FFyEvN2TXy" alt="" /></p>

<p>This post is a textual version of a talk I gave at <a href="https://events.ccc.de/congress/2024/infos/startpage.html">The 38th Chaos Computer Congress</a> at the end of 2018</p>
              </div>
            </article>
        <hr class="entry_sep">
          <article>
            <h2 class="entry_header">
              <a href="https://leahneukirchen.org/blog/archive/2024/12/merry-christmas.html">
                Merry Christmas!
                </a>
            </h2>
            <div class="entry_meta">
              <date>
                <span>24.12.2024 15:21</span>
                </date>
              &mdash; <span class="entry_author">Leah Neukirchen</span>
              </div>

            <div class="entry_content">
                <p style="margin: 1em; font-weight: bold; text-align: center">
<img src="https://leahneukirchen.org/blog/graphics/xmas2024.gif"
     alt="Animated comic christmas tree with lights and  antlers">
</p>

<p style="margin: 1em; position: relative: top: 10px; font-size: 140%; font-weight: bold; text-align: center">
Frohe Weihnachten, ein schönes Fest, und einen guten Rutsch ins neue Jahr
wünscht euch<br>Leah Neukirchen
</p>

<p style="margin: 1em; font-size: 140%; font-weight: bold; text-align: center">
Merry Christmas and a Happy New Year!
</p>

<p><small>NP: Trembling Bells&#8212;Willows Of Carbeth</small></p>
              </div>
            </article>
        <hr class="entry_sep">
          <article>
            <h2 class="entry_header">
              <a href="https://leahneukirchen.org/blog/archive/2024/12/how-to-properly-shut-down-a-linux-system.html">
                How to properly shut down a Linux system
                </a>
            </h2>
            <div class="entry_meta">
              <date>
                <span>20.12.2024 13:51</span>
                </date>
              &mdash; <span class="entry_author">Leah Neukirchen</span>
              </div>

            <div class="entry_content">
                <p>In a previous post, I discussed <a href="/blog/archive/2022/01/how-to-check-you-re-in-the-initial-pid-namespace.html">how you can determine that you are
pid 1</a>,
the init process, when the system is booting.  Today, we’ll consider
the end of the init process: system shutdown.</p>
<p>If you look into a book on Unix system administration, the classic way
to manually turn off a Unix system contains a few steps:</p>
<ol>
<li>Bring the system to single-user mode (<code>init 1</code> or <code>shutdown</code>).</li>
<li>Unmount all filesystems except for /.</li>
<li>Remount the root file system read-only.</li>
<li>Run <code>sync</code>.</li>
<li>Turn off the system.</li>
</ol>
<p>What step 1 essentially does is is kill all processes (except for pid
1), and spawn a new shell.  (Unix doesn’t have a concept of
“single-user mode” in kernel space.)  This is necessary to orderly
stop all daemons, kill all remaining user processes, and close open
files that would stop step 2 from progressing.</p>
<p>Step 3 is necessary to ensure the root file system is in a consistent
state.  Since we cannot unmount it (we still use it!), remounting it
read-only is the best available way to ensure consistency.</p>
<p>Finally, we flush buffers, and then it’s safe to turn off the machine.</p>
<p>Now, since this is
<a href="https://github.com/leahneukirchen/sabotage/blob/master/KEEP/etc/rc.shutdown">not</a>
<a href="https://github.com/leahneukirchen/ignite/blob/master/ignite/etc/runit/3">my</a>
<a href="https://github.com/void-linux/void-runit/tree/master/shutdown.d">first</a>
rodeo with writing custom init scripts, I’ve implemented these steps a bunch
of times and found out some things which were not obvious.</p>
<p>So let’s see how some of this works in detail.</p>
<h4>Killing all processes</h4>
<p>This sounds easy, but is tricky to get right.  If you are <em>not</em> pid 1,
using <code>kill(-1, SIGTERM)</code> will send SIGTERM to all processes except for
pid 1 <a href="https://github.com/torvalds/linux/blob/e84a3bf7f4aa669c05e3884497774148ac111468/kernel/signal.c#L1567">and itself</a>
(on Linux on the BSDs).  You should then send a SIGCONT to all processes, so
stopped processes will wake up and handle the SIGTERM, too.  Then you
usually wait a bit for their graceful shutdown and run <code>kill(-1, SIGKILL)</code>
to kill the rest.  Only two processes, you and init, should
remain.  The main problem with this is you don’t know when all
processes have exited after the first kill, so there’s necessarily a
delay.</p>
<p>It is therefore better to let pid 1 do the killing and reaping.  Again
we run <code>kill(-1, SIGTERM); kill(-1, SIGCONT)</code>, and then do the usual
reaping an init process should do.  When <code>waitpid</code> fails with ECHILD,
we know there’s no child left over.  Else, after some timeout, you fall
back to sending SIGKILL to the rest, and reap again.</p>
<p>(As a historical aside, Alan Cox <a href="https://mastodon.social/@etchedpixels/113631181839433174">pointed
out</a> that
on Unix V7, wait(2) in pid 1 keeps waiting for itself, since the parent of pid 1
is pid 1 itself.  However all contemporary systems deal with this fine.)</p>
<p>In the real world, you still want a timeout here.  A process could
be stuck in state <code>D</code> and not respond to SIGKILL either.  We still
want to power down at some point and not lock up shutdown due to this.</p>
<h4>Remounting the file system read-only</h4>
<p>On Linux, you do this by calling <code>mount -o remount,ro /</code>
or the equivalent syscall <code>mount(&quot;/&quot;, &quot;/&quot;, &quot;&quot;, MS_REMOUNT | MS_RDONLY, &quot;&quot;)</code>.
This can fail when the “file system is still in use” with error code
EBUSY.</p>
<p>I ran into this EBUSY error a few times before, and lately a lot
during development, and I finally tried to track down why it happens.
Usually, it’s caused by some process that still has a file handle
open, but at this point of shutdown, there’s nothing running anymore
except for our init itself, so how can that fail?</p>
<p>At first I thought it was just some erratic behavior (race condition
etc.), but then I realized I could trigger the error each time I
updated init (which happens a lot when you are testing code…).
However, when I didn’t update init, everything shutdown fine!</p>
<p>Now, I update init in my testing VM like this:</p>
<pre><code>scp leah@10.0.2.2:prj/.../init /bin/init- &amp;&amp; mv /bin/init- /bin/init
</code></pre>
<p>We can’t overwrite <code>/bin/init</code> directly, else we get ETXTBSY.  So we
do the usual dance of atomically renaming the file into the
destination, similar to how package managers do it.</p>
<p>On an inode level, what does this do?  Overwriting the <code>/bin/init</code>
file decrements the <code>st_nlink</code> field, usually to 0, which means the
old file is deleted.  However, as the init binary is still running (of
course), the inode is kept alive.  We can verify this:</p>
<pre><code># stat -L /proc/1/exe
  File: /proc/1/exe
  Size: 193032    	Blocks: 384        IO Block: 4096   regular file
Device: 8,1	Inode: 137195      Links: 0
Access: (0755/-rwxr-xr-x)  Uid: (    0/    root)   Gid: (    0/    root)
Access: 2024-12-19 17:29:21.289000000 +0000
Modify: 2024-12-19 17:29:21.302000000 +0000
Change: 2024-12-20 15:18:14.480000000 +0000
 Birth: 2024-12-19 17:29:21.289000000 +0000
</code></pre>
<p>The link count is zero indeed.</p>
<p>But this causes the file system <a href="https://github.com/torvalds/linux/blob/8faabc041a001140564f718dabe37753e88b37fa/fs/namespace.c#L710">to stay busy</a>,
since it wants to delete the file when it will be closed, so it cannot
be remounted read-only while there are open file handles to deleted files!
(Thanks to Simon Richter for <a href="https://hachyderm.io/@GyrosGeier/113679452571992737">explaining
this</a>.)  This is
also the reason for the occasional shutdown issues I had using
<code>runit</code>—likely the <code>runit</code> binary was updated during the uptime.</p>
<p>I tried many ways to work around this (old posts may suggest we can
perhaps link <code>/proc/1/exe</code> back into a file, but this behavior has been
forbidden in Linux since 2011), but ultimately I think this is a
policy problem and not one of pid 1 itself.  I therefore suggest a
simple workaround that users of other init systems can use as well:
in the startup scripts, after the root filesystem is mounted writable,
we just make a backup link for the currently booted init:</p>
<pre><code>ln -f /sbin/init /sbin/.init.old
</code></pre>
<p>This ensures that even when <code>/sbin/init</code> is overwritten, its link
count doesn’t drop to zero and we don’t block remounting read-only,
preventing a clean shutdown.</p>
<p>That’s it for now, let’s see what other surprises appear in the future.</p>
<p><small>NP: Laura Jane Grace—Punk Rock In Basements</small></p>
              </div>
            </article>
        <hr class="entry_sep">
          <article>
            <h2 class="entry_header">
              <a href="https://fzakaria.com/2024/12/18/faking-incremental-docker-loads.html">
                Faking incremental Docker loads
                </a>
            </h2>
            <div class="entry_meta">
              <date>
                <span>18.12.2024 20:21</span>
                </date>
              &mdash; <span class="entry_author">unknown</span>
              </div>

            <div class="entry_summary">
                While testcontainers have made it simple to run containers for unit &amp; system tests, they are not well suited for Bazel as they rely on docker pull to hydrate the Docker daemon. The pulls rely on tags which may be rewritten and require input from data (i.e, the images themselves) unknown to Bazel, as well as network access.
              </div>
            <div class="entry_content">
                <p>While <a href="https://testcontainers.com/">testcontainers</a> have made it simple to run containers for unit &amp; system tests, they are not well suited for <a href="https://bazel.build/">Bazel</a> as they rely on <code class="language-plaintext highlighter-rouge">docker pull</code> to hydrate the Docker daemon. The pulls rely on tags which may be rewritten and require input from data (i.e, the images themselves) unknown to Bazel, as well as network access.</p>

<!--more-->

<p><code class="language-plaintext highlighter-rouge">rules_oci</code> is a popular Bazel rules library to incorporate Docker (OCI) images into Bazel that can be used to build subsequent images or be passed as depenedencies to targets.</p>

<p>I wrote a small example <a href="https://github.com/fzakaria/bazel-testcontainer-example">https://github.com/fzakaria/bazel-testcontainer-example</a> that demonstrates how you can <em>modify</em> <a href="https://java.testcontainers.org/">testcontainers-java</a> to leverage these images by passing in the <code class="language-plaintext highlighter-rouge">tar.gz</code> of the image as a <code class="language-plaintext highlighter-rouge">data</code> dependency and explicitly loading it at startup.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">java_test</span><span class="p">(</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">"TestContainerExampleTest"</span><span class="p">,</span>
    <span class="n">srcs</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">"TestContainerExampleTest.java"</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">":tarball.tar"</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="n">env</span> <span class="o">=</span> <span class="p">{</span><span class="s">"TARBALL_RUNFILE"</span><span class="p">:</span> <span class="s">"$(rlocationpath :tarball.tar)"</span><span class="p">},</span>
    <span class="n">runtime_deps</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">"@maven//:org_slf4j_slf4j_simple"</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="n">deps</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">"@bazel_tools//tools/java/runfiles"</span><span class="p">,</span>
        <span class="s">"@maven//:org_testcontainers_testcontainers"</span><span class="p">,</span>
    <span class="p">],</span>
<span class="p">)</span>

<span class="n">tar</span><span class="p">(</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">"layer"</span><span class="p">,</span>
    <span class="n">srcs</span> <span class="o">=</span> <span class="p">[</span><span class="s">"PingService_deploy.jar"</span><span class="p">],</span>
<span class="p">)</span>

<span class="n">oci_image</span><span class="p">(</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">"image"</span><span class="p">,</span>
    <span class="n">base</span> <span class="o">=</span> <span class="s">"@distroless_java"</span><span class="p">,</span>
    <span class="n">entrypoint</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">"java"</span><span class="p">,</span>
        <span class="s">"-jar"</span><span class="p">,</span>
        <span class="s">"/src/PingService_deploy.jar"</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="n">tars</span> <span class="o">=</span> <span class="p">[</span><span class="s">":layer"</span><span class="p">],</span>
<span class="p">)</span>
</code></pre></div></div>

<p><em>Sounds great?</em> 🙌 … <em>Right?</em> 😕</p>

<p>Turns out if your image is moderately large (&gt;2GiB), an individual upload can take a relatively long time (~30s). This can compound if you have multiple concurrent tests each tryin to upload to the Docker daemon such as in the case in Bazel.</p>

<p>There is <strong>no handshaking</strong> or range-read of the compressed stream, meaning you must send the whole compressed image, which must then be uncompressed and validated for Docker to determine it already had the necessary layers present.</p>

<p>We experienced this with our tests either failing or timing out as each concurrent test tried to upload multi-gigabyte images concurrently.</p>

<p>Turns out, this limitation is documented and known:</p>
<ul>
  <li><a href="https://github.com/docker/buildx/issues/107">docker/buildx/issues/107</a></li>
  <li><a href="https://github.com/bazel-contrib/rules_oci/issues/454">bazel-contrib/rules_oci/issues/454</a></li>
  <li><a href="https://github.com/moby/moby/issues/44369">moby/moby/issues/44369</a></li>
</ul>

<p>❗ <em>There exists no API to query the layers the Docker engine has locally</em>.</p>

<p>For a <em>quick’n’dirty</em> (but effective) workaround I relied on the following before our CI job</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> bazel query <span class="s2">"kind(oci_load, //...)"</span> <span class="se">\</span>
    | xargs <span class="nt">-n</span> 1 <span class="nt">-P</span> 8 <span class="nt">-I</span> target bazel run target
</code></pre></div></div>

<p>It would be great if we didn’t need any invocation prior to a test; are Bazel users left <em>holding the bag</em> ? 🫂</p>

<p>Don’t despair! Turns out we can <strong>fake incrementality</strong> uploads in Docker with a relatively ingenious method. 😭</p>

<blockquote>
  <p>Note: I did not invent this solution. There are other existing prior art, namely:</p>
  <ul>
    <li><a href="https://github.com/bazelbuild/rules_docker/blob/master/container/incremental_load.sh.tpl">bazelbuild/rules_docker/blob/master/container/incremental_load.sh.tpl</a></li>
    <li><a href="https://github.com/aspect-build/bazel-examples/blob/main/oci_python_image/hello_world/app_test.py">aspect-build/bazel-examples/blob/main/oci_python_image/hello_world/app_test.py</a></li>
    <li><a href="https://github.com/datahouse/bazel_buildlib/blob/oss/buildlib/private/docker/src/loadImageToDocker.ts">datahouse/bazel_buildlib/blob/oss/buildlib/private/docker/src/loadImageToDocker.ts
</a></li>
  </ul>
</blockquote>

<p>🪄 The trick is that we will upload Docker images with <strong>metadata but no actual layer data</strong>, and incrementally include the layer only if it’s required.</p>

<p><img src="/assets/images/piccard_docker_image.jpg" alt="Piccard graphic" /></p>

<p>Let’s break it down.</p>

<ol>
  <li>
    <p>A Docker image, which is different than the OCI format, is a <em>tar</em> file (or <em>tar.gz</em>) with a file <code class="language-plaintext highlighter-rouge">manifest.json</code> that dictates the files that should be present within the archive.</p>

    <p>I’ve shortened the sha256 in the below example.</p>

    <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w"> </span><span class="p">[{</span><span class="w">
 </span><span class="nl">"Config"</span><span class="p">:</span><span class="w"> </span><span class="s2">"blobs/sha256/8f73f04"</span><span class="p">,</span><span class="w">
 </span><span class="nl">"RepoTags"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="s2">"example:0.1"</span><span class="w"> </span><span class="p">],</span><span class="w">
 </span><span class="nl">"Layers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
   </span><span class="s2">"blobs/sha256/6dd6992"</span><span class="p">,</span><span class="w">
   </span><span class="s2">"blobs/sha256/41e9df2"</span><span class="p">,</span><span class="w">
   </span><span class="s2">"blobs/sha256/3ec46cfe"</span><span class="p">,</span><span class="w">
   </span><span class="s2">"blobs/sha256/1225e888"</span><span class="p">,</span><span class="w">
 </span><span class="p">]}]</span><span class="w">
</span></code></pre></div>    </div>
  </li>
  <li>
    <p>Although our metadata outlines <em>4 different layers</em>, we can can omit the actual layer data.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="o">&gt;</span> <span class="nb">tar </span>tf testimage.tar.gz | tree <span class="nt">--fromfile</span> <span class="nb">.</span>
 <span class="nb">.</span>
 ├── blobs
 │   └── sha256
 │       └── 8f73f04
 └── manifest.json
</code></pre></div>    </div>
  </li>
  <li>
    <p>If we try to upload this image, if the local daemon has all the layers already present, the upload will succeed <strong>despite us not including any actual layers</strong>.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="o">&gt;</span> docker load &lt; testimage.tar.gz
 Loaded image: example:0.1
</code></pre></div>    </div>
  </li>
  <li>
    <p>If a layer is missing locally, we detect it via the error response and subsequently include it in
the archive and re-upload it.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="o">&gt;</span> docker load &lt; testimage.tar.gz
 open /var/lib/docker/tmp/docker-import-2494045611/blobs/sha256/6dd6992:
 no such file or directory
</code></pre></div>    </div>
  </li>
</ol>

<p>We can perform these steps incrementally by adding each layer one-at-a-time which looks like the following
in pseudocode.</p>

<blockquote>
  <p>⚠️ It’s important to also restrict the <code class="language-plaintext highlighter-rouge">diff_ids</code> which represent a validation of the state of the container
when the layers are applied.</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">function</span> <span class="n">incremental_load</span><span class="p">(</span><span class="n">client</span><span class="p">,</span> <span class="n">repo_tag</span><span class="p">,</span> <span class="n">base_path</span><span class="p">):</span>
<span class="s">"""Incrementally loads a Docker image."""</span>

<span class="c1"># Parse image index
</span><span class="n">index_path</span> <span class="o">=</span> <span class="n">base_path</span> <span class="o">+</span> <span class="s">"/index.json"</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">from_json</span><span class="p">(</span><span class="n">index_path</span><span class="p">)</span>

<span class="c1"># Parse manifest and config
</span><span class="n">manifest_digest</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="n">manifests</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">digest</span>
<span class="n">manifest</span> <span class="o">=</span> <span class="n">from_json</span><span class="p">(</span><span class="n">blob</span><span class="p">(</span><span class="n">base_path</span><span class="p">,</span> <span class="n">manifest_digest</span><span class="p">))</span>
<span class="n">full_config</span> <span class="o">=</span> <span class="n">from_json</span><span class="p">(</span><span class="n">blob</span><span class="p">(</span><span class="n">base_path</span><span class="p">,</span> <span class="n">manifest</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">digest</span><span class="p">))</span>
<span class="n">config_blob_path</span> <span class="o">=</span> <span class="n">blob_path</span><span class="p">(</span><span class="n">manifest</span><span class="p">.</span><span class="n">config</span><span class="p">)</span>

<span class="n">missing_layer</span> <span class="o">=</span> <span class="n">null</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">manifest</span><span class="p">.</span><span class="n">layers</span><span class="p">):</span>
  <span class="c1"># Try uploading each layer one at a time
</span>  <span class="n">layers</span> <span class="o">=</span> <span class="n">manifest</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>

  <span class="c1"># Create partial config
</span>  <span class="n">tmp_config</span> <span class="o">=</span> <span class="n">full_config</span><span class="p">.</span><span class="n">clone</span><span class="p">().</span><span class="n">rootfs</span><span class="p">.</span><span class="n">diffIds</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>

  <span class="c1"># Create partial image tar
</span>  <span class="n">image</span> <span class="o">=</span> <span class="n">create_image_tar</span><span class="p">(</span><span class="n">base_path</span><span class="p">,</span> <span class="n">config_blob_path</span><span class="p">,</span>
                           <span class="n">tmp_config</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">missing_layer</span><span class="p">)</span>

  <span class="c1"># Upload partial image, and parse out if any layer is needed
</span>  <span class="n">missing_layer</span> <span class="o">=</span> <span class="n">upload_image</span><span class="p">(</span><span class="n">client</span><span class="p">,</span> <span class="n">image</span><span class="p">)</span>

  <span class="c1"># No missing layer, move onto the next one
</span>  <span class="k">if</span> <span class="n">missing_layer</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># Missing layer found, try again but this time upload it!
</span>    <span class="k">pass</span>

<span class="c1"># Upload full image
</span><span class="n">full_image</span> <span class="o">=</span> <span class="n">create_image_tar</span><span class="p">(</span><span class="n">base_path</span><span class="p">,</span> <span class="n">config_blob_path</span><span class="p">,</span>
                              <span class="n">full_config</span><span class="p">,</span> <span class="n">manifest</span><span class="p">.</span><span class="n">layers</span><span class="p">)</span>
<span class="n">upload_image</span><span class="p">(</span><span class="n">client</span><span class="p">,</span> <span class="n">full_image</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>If you are interested in the equivalent Java code let me know and I can publish it.</p>
</blockquote>

<p>With this approach you can now have <strong>incremental Docker uploads</strong>! Huzzah! 🙌🏽</p>

<p>Problem solved? Sorta? Well….not actually. If the images you are uploading contain
individual large layers, perhaps they were squashed, we are back to square one.</p>

<p>Here we see an example image whose single layer is 1.28GiB.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> docker image <span class="nb">history </span>bad_example:0.1 <span class="nt">--human</span> <span class="se">\</span>
                    <span class="nt">--format</span> <span class="s1">'table '</span> | <span class="nb">head
</span>SIZE
0B
0B
7.87kB
0B
1.28GB
0B
0B
0B
0B
</code></pre></div></div>

<h3 id="wheres-time-spent">Where’s time spent?</h3>
<p>At this point you have to improve the image by seggregating the data into more multiple layers or continue to upload it outside of the Bazel context.</p>

<p>🕵️ I would like to dive deeper and understand why the uploads completely stall.</p>

<p>The relevant code in Docker <a href="https://github.com/moby/moby/blob/0d53725a7f8abb0b75961806da252f31155cb813/image/tarexport/load.go#L33">can be found here</a>.</p>

<p>Quick benchmarks done on my M3 Pro MacBook demonstrate it takes ~35-45 seconds to gzip a 2GiB file.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="nb">time </span>docker save bad_example:0.1 | <span class="nb">gzip</span> <span class="o">&gt;</span> test.tar.gz
docker save bad_example:0.1  0.49s user 2.49s system 6% cpu 44.843 total
<span class="nb">gzip</span> <span class="o">&gt;</span> test.tar.gz  36.72s user 0.48s system 82% cpu 44.842 total
</code></pre></div></div>

<p>Uploading the image seems to take ~15 seconds</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="nb">time </span>docker load &lt; test.tar.gz
75cc828c731c: Loading layer <span class="o">[==================================================&gt;]</span>  102.1MB/102.1MB
20ebbf9559c4: Loading layer <span class="o">[==================================================&gt;]</span>  552.9MB/552.9MB
1049fe83b46b: Loading layer <span class="o">[==================================================&gt;]</span>  10.14MB/10.14MB
b4a5b99cb981: Loading layer <span class="o">[==================================================&gt;]</span>  331.8kB/331.8kB
a9e2a3aa94a5: Loading layer <span class="o">[==================================================&gt;]</span>  39.34MB/39.34MB
93ca7c014948: Loading layer <span class="o">[==================================================&gt;]</span>  6.144kB/6.144kB
71d670ccc47b: Loading layer <span class="o">[==================================================&gt;]</span>  4.608kB/4.608kB
1838b4d29208: Loading layer <span class="o">[==================================================&gt;]</span>  2.048kB/2.048kB
0d9eb9b0c742: Loading layer <span class="o">[==================================================&gt;]</span>   2.56kB/2.56kB
c68e52b834e4: Loading layer <span class="o">[==================================================&gt;]</span>  1.284GB/1.284GB
749f1729f609: Loading layer <span class="o">[==================================================&gt;]</span>   16.9kB/16.9kB
Loaded image: bad_example:0.1
docker load &lt; test.tar.gz  0.38s user 1.62s system 13% cpu 14.565 total
</code></pre></div></div>

<p>That means creating the archive  and uploading it can take ~1 minute of test execution time. This problem seems to compound with multiple archives created and uploaded; more research is needed to know if the bottleneck is the Docker daemon itself (a global lock?) or the I/O of the disk.</p>
              </div>
            </article>
        <hr class="entry_sep">
          <article>
            <h2 class="entry_header">
              <a href="https://blog.benjojo.co.uk/post/rfc-in-38-simple-steps">
                The "simple" 38 step journey to getting an RFC
                </a>
            </h2>
            <div class="entry_meta">
              <date>
                <span>05.12.2024 10:43</span>
                </date>
              &mdash; <span class="entry_author">author</span>
              </div>

            <div class="entry_summary">
                <h1>The &ldquo;simple&rdquo; 38 step journey to getting an RFC</h1>

<p>The Internet is built on the mutual understanding of network protocols and practices, and most of those protocols are defined using Request For Comments (RFC) or Best Common Practices (BCP) documents.</p>
              </div>
            </article>
        <hr class="entry_sep">
          <article>
            <h2 class="entry_header">
              <a href="https://leahneukirchen.org/blog/archive/2024/11/time-series-based-monitoring-in-very-heterogeneous-environments.html">
                Time series based monitoring in very heterogeneous environments
                </a>
            </h2>
            <div class="entry_meta">
              <date>
                <span>29.11.2024 18:49</span>
                </date>
              &mdash; <span class="entry_author">Leah Neukirchen</span>
              </div>

            <div class="entry_content">
                <p>For the last few years, I have built a centralized monitoring system
based on <a href="https://prometheus.io/">Prometheus</a> that gathers various
metrics across my whole private fleet of servers.</p>
<p>Since writing Prometheus exporters is rather simple, I have written
some of them myself:</p>
<ul>
<li><a href="https://github.com/leahneukirchen/lywsd03mmc-exporter/">lywsd03mmc-exporter</a>,
a Prometheus exporter for the LYWSD03MMC BLE
thermometer which monitors my flat’s temperature and air humidity
(as well as when to replace the batteries).</li>
<li><a href="https://leahneukirchen.org/dotfiles/bin/card10-bme680-exporter.rb">card10-bme680-exporter</a>,
which accesses the environmental sensor of the <a href="https://card10.badge.events.ccc.de/">card10</a>
via serial port for measuring air quality.</li>
<li><a href="https://leahneukirchen.org/dotfiles/bin/tab-exporter.rb">tab-exporter</a>
which exports the number of Firefox tabs I have open
(this needs <a href="https://git.yori.cc/yorick/dotfiles/src/branch/master/pkgs/countfftabs">countfftabs</a>).</li>
<li>I also forked and improved <a href="https://github.com/leahneukirchen/nano-exporter/">nano-exporter</a>,
a very lightweight and zero-dependency Linux version of <a href="https://github.com/prometheus/node_exporter">node_exporter</a>.</li>
</ul>
<p>Additionally I use the following pre-made exporters:</p>
<ul>
<li><a href="https://github.com/prometheus/node_exporter">node_exporter</a> for monitoring FreeBSD and some other hosts.</li>
<li><a href="https://github.com/martin-helmich/prometheus-nginxlog-exporter">prometheus-nginxlog-exporter</a> for web server metrics.</li>
<li><a href="https://github.com/SuperQ/chrony_exporter">chrony_exporter</a> for monitoring my NTP server.</li>
<li><a href="https://github.com/brendanbank/gpsd-prometheus-exporter">gpsd-prometheus-exporter</a> for monitoring the GPS signal on my NTP server.</li>
<li><a href="https://github.com/SuperQ/smokeping_prober">smokeping_prober</a> for detecting network outages and other problems.</li>
<li><a href="https://github.com/google/mtail">mtail</a> for extracting metrics out of Postfix logs and other log files.</li>
</ul>
<p>As you can see, this is quite a lot of different exporters running on
different hosts.</p>
<p>A few months ago I decided to rebuild the centralized metrics server on
top of <a href="https://victoriametrics.com/">VictoriaMetrics</a> and with proper
access control.</p>
<p>Why VictoriaMetrics? I tried it for a bit and it seems to use less RAM
and less storage while supporting long term storage nicely.  It also has
better mechanisms for importing and exporting data than Prometheus.</p>
<h5>Setting up VictoriaMetrics</h5>
<p>Setting up <code>victoria-metrics</code> is very easy.  I run it like this:</p>
<pre><code>victoria-metrics -enableTCP6 \
  -storageDataPath=/srv/victoria-metrics \
  -retentionPeriod=99y \
  -httpListenAddr=127.0.0.1:8428 \
  -selfScrapeInterval=20s \
  -promscrape.config /usr/local/etc/prometheus/prometheus.yaml
</code></pre>
<p>Note that you need to enable IPv6 manually all the time.</p>
<p>The <code>prometheus.yaml</code> file is compatible with stock Prometheus.</p>
<p>I then use Grafana to connect to it, using the Prometheus protocol.</p>
<h5>Scraping non-public endpoints</h5>
<p>I don’t consider most of above metrics to be super private, but they
certainly leak metadata (e.g. am I at home or not, how much mail do I
get) so I don’t want to publish them on the net accessible to everyone
that finds them.</p>
<p>Since Prometheus mainly favors a pull based model, we need to figure
out ways to protect the data.</p>
<p>“Obvious” solutions like using mTLS or a maintenance VPN would require
reconfiguring many machines and were deemded too much effort.</p>
<p>Essentially, I found three solutions that I will describe in detail:</p>
<h4>Hiding metrics behind existing web servers</h4>
<p>This is the easiest mechanism, when your host already runs a web
server: simply use it as a proxy for the metrics, and filter access by
IP address or Basic Auth.  Since most webservers have HTTPS today
already, you get encryption for free.</p>
<p>A simple nginx configuration to do this would be:</p>
<pre><code>location /metrics {
        proxy_http_version 1.1;
        proxy_pass http://127.0.0.1:9100/metrics;
        access_log off;
        allow 127.0.0.1;
        allow ...;
        deny all;
}
</code></pre>
<p>You need to configure the metrics exporter to only listen on localhost.</p>
<h5>Reverse SSH tunnelling</h5>
<p>This is a quite elegant solution that provides encryption, flexible
configuration, and can be used when the scrape target doesn’t have a
public IP address.  OpenSSH provides the <code>-R</code> flag to do reverse port
forwarding, but most people don’t know it also can be used to run a
reverse SOCKS proxy!</p>
<p>For this, I create a separate Unix user on scrape target and server,
and assign it a SSH key.  Then, the target runs:</p>
<pre><code>ssh -o ServerAliveInterval=15 -o ExitOnForwardFailure=yes -R8083 server.example.com -NT
</code></pre>
<p>You should run this using <a href="https://smarden.org/runit/">service supervision</a>
so it tries to reconnect on network failures.</p>
<p>On the server side, you restrict access to only open a port
using <code>/etc/ssh/authorized_keys/scrape-user</code>:</p>
<pre><code>restrict,port-forwarding,permitlisten=&quot;8083&quot; ssh-ed25519 ....
</code></pre>
<p>Then, the server can use port 8083 as a SOCKS proxy to access the
network of the scrape target directly!  So you can write a scrape config like:</p>
<pre><code>  - job_name: 'nano-exporter-hecate'
    proxy_url: 'socks5://127.0.0.1:8083'
    static_configs:
    - targets: ['127.0.0.1:9100']
      labels:
        instance: 'hecate.home.vuxu.org:9100'
    - targets: ['10.0.0.119:9100']
      labels:
        instance: 'leto.home.vuxu.org:9100'
</code></pre>
<p>Here, we use a host in my home network that is always on, and can also
safely scrape other hosts in the same LAN.
(Note that the IP addresses in <code>targets</code> are resolved relative to the SSH client.)</p>
<h5>Pushing with vmagent</h5>
<p>I used the SSH approach for my notebook as well, but there’s the
problem that we lose data when there’s no Internet connection
available.  I have thus moved my notebook to a solution using
<a href="https://docs.victoriametrics.com/vmagent/"><code>vmagent</code></a>, which is
included with VictoriaMetrics.</p>
<p><code>vmagent</code> does scrape metrics just like VictoriaMetrics (and also
supports all other metrics protocols, but I don’t use them), but it
simply forwards everything via the Prometheus remote write protocol,
and locally buffers data if it can’t forward the metrics currently.</p>
<p>On the server side, we need to provide access to the remote write
protocol.  Since VictoriaMetrics operates without internal access
control, we can use the
<a href="https://docs.victoriametrics.com/vmauth/"><code>vmauth</code></a> gateway to
implement Basic Auth over TLS.  (Again, you can use an existing HTTPS
server and proxy it, but in this case I don’t have a HTTPS server on
the metrics host.)</p>
<p><code>vmauth</code> needs some configuration.  First, we create a self-signed
certificate (Let’s Encrypt support is limited to the commercial
version of VictoriaMetrics unfortunately):</p>
<pre><code>openssl req -x509 -newkey ed25519 \
	-keyout /usr/local/etc/vmauth/key.pem \
	-out /usr/local/etc/vmauth/cert.pem \
	-sha256 -days 3650 -nodes -subj &quot;/CN=server.example.org&quot; \
	-addext &quot;subjectAltName = DNS:server.example.org&quot;
</code></pre>
<p>I then run it as:</p>
<pre><code>vmauth -enableTCP6 \
  -tls \
  -tlsCertFile=/usr/local/etc/vmauth/cert.pem \
  -tlsKeyFile=/usr/local/etc/vmauth/key.pem \
  -reloadAuthKey=secret \
  -flagsAuthKey=secret \
  -metricsAuthKey=secret \
  -pprofAuthKey=secret \
  -auth.config=/usr/local/etc/vmauth/vmauth.yaml
</code></pre>
<p>(I think it’s unfortunate that we need to add auth-keys now,
as internal and forwarded API are exposed on the same port…)</p>
<p>The <code>vmauth.yaml</code> configures who can access:</p>
<pre><code>users:
- username: &quot;client&quot;
  password: &quot;evenmoresecret&quot;
  url_prefix: &quot;http://localhost:8428/&quot;
</code></pre>
<p>Here, <code>localhost:8428</code> is the VictoriaMetrics instance.</p>
<p>Finally, on the scrape target we can now run <code>vmagent</code>:</p>
<pre><code>vmagent -enableTCP6 \
        -promscrape.config=/etc/vmagent/promscrape.yml \
        -httpListenAddr=127.0.0.1:8429 \
        -remoteWrite.url=https://server.example.org:8427/api/v1/write \
        -remoteWrite.label=vmagent=myhostname \
        -remoteWrite.retryMinInterval=30s \
        -remoteWrite.basicAuth.username=client \
        -remoteWrite.basicAuth.passwordFile=/etc/vmagent/passwd \
        -remoteWrite.tlsCAFile=/etc/vmagent/cert.pem
</code></pre>
<p>The <code>cert.pem</code> is copied from the server, the password is stored in <code>/etc/vmagent/passwd</code>.</p>
<p>Note that the <code>vmagent</code> instance is configured locally, so we can again
scrape targets that are only reachable from it.  We also can adjust
the scrape targets without having to touch the metrics server itself.</p>
<p><small>NP: Godspeed You! Black Emperor—Broken Spires At Dead Kapital</small></p>
              </div>
            </article>
        <hr class="entry_sep">
          <article>
            <h2 class="entry_header">
              <a href="https://fzakaria.com/2024/11/28/bazel-knowledge-protobuf-is-the-worst-when-it-should-be-the-best.html">
                Bazel Knowledge: Protobuf is the worst when it should be the best
                </a>
            </h2>
            <div class="entry_meta">
              <date>
                <span>28.11.2024 22:07</span>
                </date>
              &mdash; <span class="entry_author">unknown</span>
              </div>

            <div class="entry_summary">
                Bazel has always had support for protocol buffers (protobuf) since the beginning. Both being a Google product, one would think that their integration would be seamless and the best experience. Unfortunately, it’s some of the worst part of the user experience with Bazel I’ve found. 😔
              </div>
            <div class="entry_content">
                <p>Bazel has always had support for <a href="https://protobuf.dev/">protocol buffers (protobuf)</a> since the beginning.
Both being a Google product, one would think that their integration would be seamless and the best experience.
Unfortunately, it’s some of the worst part of the user experience with Bazel I’ve found. 😔</p>

<!--more-->

<p>Let’s start with the basics; <em>What rule should I adopt for protobufs?</em></p>

<p>Well first I Google <em>“Bazel protobuf”</em> and land on the <a href="https://bazel.build/reference/be/protocol-buffer">protobuf reference page</a> for Bazel
which states:</p>

<blockquote>
  <p>If using Bazel, please load the rule from https://github.com/bazelbuild/rules_proto.</p>
</blockquote>

<p>One may think the sensible <a href="https://github.com/bazelbuild/rules_proto">rules_proto</a> is a good starting
point but the <em>README.md</em> states:</p>

<blockquote>
  <p>This repository is <strong>deprecated</strong>…we decided to move the implementation of
the rules together with proto compiler into protobuf repository.</p>
</blockquote>

<p>OK…🤔</p>

<p>Let’s go check <a href="https://github.com/protocolbuffers/protobuf">protobuf</a>.</p>

<p>The <em>README.md</em> claims one can install one of two ways by inserting the following
into your <em>MODULE.bazel</em> without much explanation as to the difference. 🤷‍♂️</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bazel_dep</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s">"protobuf"</span><span class="p">,</span> <span class="n">version</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">VERSION</span><span class="o">&gt;</span><span class="p">)</span>
<span class="c1">#
# or
#
</span><span class="n">bazel_dep</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s">"protobuf"</span><span class="p">,</span> <span class="n">version</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">VERSION</span><span class="o">&gt;</span><span class="p">,</span>
          <span class="n">repo_name</span> <span class="o">=</span> <span class="s">"com_google_protobuf"</span><span class="p">)</span>
</code></pre></div></div>

<p>I decide to audit the source to see what’s going on.
You quickly land on the <a href="https://github.com/protocolbuffers/protobuf/blob/cbecd9d2fa1d7187cca63a8c18838e87a4f613ec/bazel/private/bazel_proto_library_rule.bzl#L239">rule definition</a>
for <em>proto_library</em> and see the following documentation for the rule. 🤦</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">proto_library</span> <span class="o">=</span> <span class="n">rule</span><span class="p">(</span>
    <span class="n">_proto_library_impl</span><span class="p">,</span>
    <span class="c1"># TODO: proto_common docs are missing
</span>    <span class="c1"># TODO: ProtoInfo link doesn't work and docs are missing
</span>    <span class="n">doc</span> <span class="o">=</span> <span class="s">"""
&lt;p&gt;If using Bazel, please load the rule from
&lt;a href="https://github.com/bazelbuild/rules_proto"&gt;
https://github.com/bazelbuild/rules_proto&lt;/a&gt;.
</span></code></pre></div></div>

<p>Where is the <strong>protoc</strong> (protobuf compiler) ultimately coming from for the rule?
I notice these interesting snippets in the rule.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">toolchains</span><span class="p">.</span><span class="n">if_legacy_toolchain</span><span class="p">({</span>
        <span class="s">"_proto_compiler"</span><span class="p">:</span> <span class="n">attr</span><span class="p">.</span><span class="n">label</span><span class="p">(</span>
            <span class="n">cfg</span> <span class="o">=</span> <span class="s">"exec"</span><span class="p">,</span>
            <span class="n">executable</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
            <span class="n">allow_files</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
            <span class="n">default</span> <span class="o">=</span> <span class="n">configuration_field</span><span class="p">(</span><span class="s">"proto"</span><span class="p">,</span> <span class="s">"proto_compiler"</span><span class="p">),</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">_incompatible_toolchain_resolution</span> <span class="o">=</span>
    <span class="nb">getattr</span><span class="p">(</span><span class="n">native_proto_common</span><span class="p">,</span>
            <span class="s">"INCOMPATIBLE_ENABLE_PROTO_TOOLCHAIN_RESOLUTION"</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_if_legacy_toolchain</span><span class="p">(</span><span class="n">legacy_attr_dict</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">_incompatible_toolchain_resolution</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{}</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">legacy_attr_dict</span>
</code></pre></div></div>

<p>Turns out that <em>INCOMPATIBLE_ENABLE_PROTO_TOOLCHAIN_RESOLUTION</em> is set from the command line <a href="https://bazel.build/reference/command-line-reference#flag--incompatible_enable_proto_toolchain_resolution">ref</a>.</p>

<blockquote>
  <p>–[no]incompatible_enable_proto_toolchain_resolution default: “false”
If true, proto lang rules define toolchains from protobuf repository.
Tags: loading_and_analysis, incompatible_change</p>
</blockquote>

<p>I don’t have that in my <code class="language-plaintext highlighter-rouge">.bazelrc</code> so let’s ignore it.
That means our <code class="language-plaintext highlighter-rouge">_proto_compiler</code> is coming from <code class="language-plaintext highlighter-rouge">configuration_field("proto", "proto_compiler")</code>.</p>

<p>You then search the <a href="https://github.com/bazelbuild/bazel/blob/a3f0cebd35989e120d5cdaf7882b4e93df82e590/src/main/java/com/google/devtools/build/lib/rules/proto/ProtoConfiguration.java#L68">bazelbuild/bazel</a> source to find where it’s defined.</p>
<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@Option</span><span class="o">(</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">"proto_compiler"</span><span class="o">,</span>
    <span class="n">defaultValue</span> <span class="o">=</span> <span class="nc">ProtoConstants</span><span class="o">.</span><span class="na">DEFAULT_PROTOC_LABEL</span><span class="o">,</span>
    <span class="n">converter</span> <span class="o">=</span> <span class="nc">CoreOptionConverters</span><span class="o">.</span><span class="na">LabelConverter</span><span class="o">.</span><span class="na">class</span><span class="o">,</span>
    <span class="n">documentationCategory</span> <span class="o">=</span> <span class="nc">OptionDocumentationCategory</span><span class="o">.</span><span class="na">UNCATEGORIZED</span><span class="o">,</span>
    <span class="n">effectTags</span> <span class="o">=</span> <span class="o">{</span><span class="nc">OptionEffectTag</span><span class="o">.</span><span class="na">AFFECTS_OUTPUTS</span><span class="o">,</span> <span class="nc">OptionEffectTag</span><span class="o">.</span><span class="na">LOADING_AND_ANALYSIS</span><span class="o">},</span>
    <span class="n">help</span> <span class="o">=</span> <span class="s">"The label of the proto-compiler."</span><span class="o">)</span>
<span class="kd">public</span> <span class="nc">Label</span> <span class="n">protoCompiler</span><span class="o">;</span>
</code></pre></div></div>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// The flags need to point to @bazel_tools, because this is a canonical repo</span>
<span class="c1">// name when either bzlmod or WORKSPACE mode is used.</span>
<span class="cm">/** Default label for proto compiler.*/</span>
<span class="kd">public</span> <span class="kd">static</span> <span class="kd">final</span> <span class="nc">String</span> <span class="no">DEFAULT_PROTOC_LABEL</span>
        <span class="o">=</span> <span class="s">"@bazel_tools//tools/proto:protoc"</span><span class="o">;</span>
</code></pre></div></div>

<p>Chasing down the ultimate target in the defining <a href="https://github.com/bazelbuild/bazel/blob/3d528ac42cce1a71d8358b57cdbe4b3e743bd307/tools/proto/BUILD#L15">BUILD</a>
file you discover it’s an alias to <code class="language-plaintext highlighter-rouge">"@com_google_protobuf//:protoc"</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Those aliases are needed to resolve the repository name correctly in both
# bzlmod and WORKSPACE mode. They are resolved in the namespace of MODULE.tools
</span>
<span class="n">alias</span><span class="p">(</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">"protoc"</span><span class="p">,</span>
    <span class="n">actual</span> <span class="o">=</span> <span class="s">"@com_google_protobuf//:protoc"</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<p>😲 So we discovered why <code class="language-plaintext highlighter-rouge">com_google_protobuf</code> may want to be the <code class="language-plaintext highlighter-rouge">repo_name</code> in the <code class="language-plaintext highlighter-rouge">bazel_dep</code> rule.
The repository name <code class="language-plaintext highlighter-rouge">com_google_protobuf</code> is <em>hard-coded</em> within the Bazel source code for the location
to discover the protoc compiler.</p>

<blockquote>
  <p>You’ll have to trust me that the resolution to the compiler for the language toolchains
such as <em>java_proto_library</em> is the same as well; just way more obfuscated.</p>
</blockquote>

<p>The rabbit hole only goes deeper if you consider <a href="https://grpc.io/">gRPC</a>, other languages and then having to manage
various runtimes (compatibility matrix) for your language across your codebases if they leave source of truth.</p>

<p>I feel like we discovered a lot but didn’t really learn or accomplish anything. 😩</p>

<h3 id="brighter-future">Brighter Future?</h3>

<p>Lots of interesting work is being done by the <a href="https://bazel-contrib.github.io/SIG-rules-authors/proto-grpc.html">rule-authors SIG</a> (Special Interest Group).</p>

<blockquote>
  <p>That doc has a great in-depth overview of the current <em>state of affairs</em>.</p>
</blockquote>

<p>The most notable changes on the horizon are migrating protocol buffers to Bazel’s toolchain mechanism.
This should make binding to <code class="language-plaintext highlighter-rouge">protoc</code> look like other toolchains in Bazel and no longer special case
<code class="language-plaintext highlighter-rouge">com_google_protobuf</code>.</p>

<blockquote>
  <p>What are toolchains? In my mind effectively the capability to late bind a label to a target.</p>
</blockquote>

<p>To me, a simple immediate improvement would be fixing the documentation around <em>rules_proto</em> and
having a more clear path on how to adopt Bazel given some constraint (i.e. Bazel &gt;= 7.0).</p>

<blockquote>
  <p>The <a href="https://blog.bazel.build/2017/02/27/protocol-buffers.html">latest blog post</a> from Bazel on protobuf
is from 2017!</p>
</blockquote>

<p>The work <a href="https://www.aspect.build/">Aspect Build</a> is doing to improve the protobuf ecosystem is great as well.
Their video series on <a href="https://www.youtube.com/watch?v=s0i_Ra_mG9U"><em>“Never Compile Protoc Again”</em></a> is excellent
and served as a great resource for my previous post on <a href="/2024/10/23/bazel-knowledge-mind-your-path.html">minding your PATH</a>.</p>
              </div>
            </article>
        <hr class="entry_sep">
          <article>
            <h2 class="entry_header">
              <a href="https://tech.j4m3s.eu/posts/git-filter-scripts/">
                Protect Your Sensitive Files in Git Repos: A Guide to Git Filters and age
                </a>
            </h2>
            <div class="entry_meta">
              <date>
                <span>10.11.2024 16:28</span>
                </date>
              </div>

            <div class="entry_summary">
                This blog post explores how to leverage Git clean/smudge filters alongside the age encryption tool to securely manage sensitive files in Git repositories. Git filters allow developers to preprocess files automatically, making them perfect for encrypting confidential data (e.g., API keys, configuration files) before committing to a repo and decrypting them upon checkout.
              </div>
            </article>
        <hr class="entry_sep">
          <article>
            <h2 class="entry_header">
              <a href="https://fzakaria.com/2024/11/08/jvm-boot-optimization-via-javaindex.html">
                JVM boot optimization via JavaIndex
                </a>
            </h2>
            <div class="entry_meta">
              <date>
                <span>08.11.2024 22:01</span>
                </date>
              &mdash; <span class="entry_author">unknown</span>
              </div>

            <div class="entry_summary">
                Ever heard of a JarIndex? I had been doing JVM development for 10+ years and I hadn’t. Read on to discover what it is and how it can speedup your compilation and boot time. 🤓 After having worked on Shrinkwrap and publishing our results in Mapping Out the HPC Dependency Chaos, you start to see the Linux environment as a bit of an oddball. Everything in Linux is structured around O(n) or O(n^2) search and lookup. This feels now unsurprising given that everything in Linux searches across colon separate lists (i.e. LD_LIBRARY_PATH, RUN_PATH). This idiom however is even more pervasive and has bled into all of our language. The JVM for instance, must search for classes amongst a set of directories, files or JARs set on the CLASS_PATH.
              </div>
            <div class="entry_content">
                <p><em>Ever heard of a JarIndex? I had been doing JVM development for 10+ years and I hadn’t. Read on to discover what it is and how it can speedup your compilation and boot time.</em> 🤓</p>

<p>After having worked on <a href="https://github.com/fzakaria/shrinkwrap">Shrinkwrap</a> and publishing our results in <a href="https://arxiv.org/abs/2211.05118">Mapping Out the HPC Dependency Chaos</a>, you start
to see the Linux environment as a bit of an oddball.</p>

<p><em>Everything in Linux is structured around O(n) or O(n^2) search and lookup</em>.</p>

<p>This feels now unsurprising given that everything in Linux searches across colon separate lists (i.e. <em>LD_LIBRARY_PATH</em>, <em>RUN_PATH</em>).
This idiom however is even more pervasive and has bled into all of our language.</p>

<p>The JVM for instance, must search for classes amongst a set of directories, files or JARs set on the <em>CLASS_PATH</em>.
<!--more--></p>

<p>Everytime the JVM needs to load a class file, it must perform a linear search along all entries in the <em>CLASS_PATH</em>.
Thanksfully, if the entries are directories or JARs, no subsequent search must be performed since the package name of a class dictates the directory structure
that must exist.</p>

<p><code class="language-plaintext highlighter-rouge">io.fzakaria.Example</code> -&gt; <code class="language-plaintext highlighter-rouge">io/fzakaria/Example.class</code></p>

<p>Nevertheless, the <em>CLASS_PATH</em> size can be large. 
At <em>$DAY_JOB$</em>, almost all of our services launch with +300 entries (JARs) on the ClassPath.</p>

<p>Large enterprise codebases may feature over a thousand ClassPath entries. 😮</p>

<p>A large ClassPath means that the JavaVirtualMachine (JVM) needs to search entry for the desired class.
This not only affects startup time for your application, <em>on every startup, repeatedly</em>, but also compilation as well via <code class="language-plaintext highlighter-rouge">javac</code>.</p>

<p>The authors of the JVM already knew about this problem, especially when the idea of Java Applets were dominant. Each JAR on the ClassPath
would have been fetched via HTTP and would cause unbearable slowdown for startup.</p>

<p>The JDK has support for a <em>JarIndex</em>.</p>

<p>A <em>JarIndex</em>, is a JAR which has a special file <code class="language-plaintext highlighter-rouge">INDEX.LIST</code> that effectively contains an index of all JARs on the ClassPath and the packages found within.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>JarIndex-Version: 1.0

libMain.jar
Main.class

lib/libA.jar
A.class

lib/libB.jar
B.class
</code></pre></div></div>

<p>Whenever a class must be searched rather than searching through the <em>CLASS_PATH</em>, the index file is used leading to constant-time lookup for classes.</p>

<p>This seemingly powerful primitive confusingly has been deprecated and ultimately removed in JDK22 (<a href="https://bugs.openjdk.org/browse/JDK-8302819">JDK-8302819</a>) 🤔 – citing challenges when having to support a broad ranges of topics such as Multi-Version JARs.</p>

<p>Unsuprisingly, I think this feature would be an easy fit into Bazel, Spack or Nix – as there are a lot more constraints on the type of JARs that need be supported.</p>

<p>I put together a small <a href="https://gist.github.com/fzakaria/4e98f65be96cf7f8b13081e75d7a2bf8">gist</a> on what this support might look like.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_jar_index_impl</span><span class="p">(</span><span class="n">ctx</span><span class="p">):</span>
    <span class="n">java_info</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">attr</span><span class="p">.</span><span class="n">src</span><span class="p">[</span><span class="n">JavaInfo</span><span class="p">]</span>
    <span class="n">java_runtime</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">attr</span><span class="p">.</span><span class="n">_java_runtime</span><span class="p">[</span><span class="n">java_common</span><span class="p">.</span><span class="n">JavaRuntimeInfo</span><span class="p">]</span>
    <span class="n">java_home</span> <span class="o">=</span> <span class="n">java_runtime</span><span class="p">.</span><span class="n">java_home</span>
    <span class="n">jar_bin</span> <span class="o">=</span> <span class="s">"%s/bin/jar"</span> <span class="o">%</span> <span class="n">java_home</span>

    <span class="n">runtime_jars</span> <span class="o">=</span> <span class="s">" "</span>
    <span class="k">for</span> <span class="n">jar</span> <span class="ow">in</span> <span class="n">java_info</span><span class="p">.</span><span class="n">transitive_runtime_jars</span><span class="p">.</span><span class="n">to_list</span><span class="p">():</span>
        <span class="n">runtime_jars</span> <span class="o">+=</span> <span class="n">jar</span><span class="p">.</span><span class="n">path</span> <span class="o">+</span> <span class="s">" "</span>

    <span class="n">cmds</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">"%s -i %s %s"</span> <span class="o">%</span> <span class="p">(</span><span class="n">jar_bin</span><span class="p">,</span> <span class="n">java_info</span><span class="p">.</span><span class="n">java_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">class_jar</span><span class="p">.</span><span class="n">path</span><span class="p">,</span> <span class="n">runtime_jars</span><span class="p">),</span>
        <span class="s">"cp %s %s"</span> <span class="o">%</span> <span class="p">(</span><span class="n">java_info</span><span class="p">.</span><span class="n">java_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">class_jar</span><span class="p">.</span><span class="n">path</span><span class="p">,</span> <span class="n">ctx</span><span class="p">.</span><span class="n">outputs</span><span class="p">.</span><span class="n">index</span><span class="p">.</span><span class="n">path</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="n">ctx</span><span class="p">.</span><span class="n">actions</span><span class="p">.</span><span class="n">run_shell</span><span class="p">(</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span> <span class="n">java_info</span><span class="p">.</span><span class="n">java_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">class_jar</span><span class="p">]</span> <span class="o">+</span> <span class="n">java_info</span><span class="p">.</span><span class="n">transitive_runtime_jars</span><span class="p">.</span><span class="n">to_list</span><span class="p">(),</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">ctx</span><span class="p">.</span><span class="n">outputs</span><span class="p">.</span><span class="n">index</span><span class="p">],</span>
        <span class="n">tools</span> <span class="o">=</span> <span class="n">java_runtime</span><span class="p">.</span><span class="n">files</span><span class="p">,</span>
        <span class="n">command</span> <span class="o">=</span> <span class="s">";</span><span class="se">\n</span><span class="s">"</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">cmds</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="p">[</span>
        <span class="n">DefaultInfo</span><span class="p">(</span><span class="n">files</span> <span class="o">=</span> <span class="n">depset</span><span class="p">([</span><span class="n">ctx</span><span class="p">.</span><span class="n">outputs</span><span class="p">.</span><span class="n">index</span><span class="p">])),</span>
    <span class="p">]</span>

<span class="n">jar_index</span> <span class="o">=</span> <span class="n">rule</span><span class="p">(</span>
    <span class="n">implementation</span> <span class="o">=</span> <span class="n">_jar_index_impl</span><span class="p">,</span>
    <span class="n">attrs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s">"src"</span><span class="p">:</span> <span class="n">attr</span><span class="p">.</span><span class="n">label</span><span class="p">(</span>
            <span class="n">mandatory</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
            <span class="n">providers</span> <span class="o">=</span> <span class="p">[</span><span class="n">JavaInfo</span><span class="p">],</span>
        <span class="p">),</span>
        <span class="s">"_java_runtime"</span><span class="p">:</span> <span class="n">attr</span><span class="p">.</span><span class="n">label</span><span class="p">(</span>
            <span class="n">default</span> <span class="o">=</span> <span class="s">"@bazel_tools//tools/jdk:current_java_runtime"</span><span class="p">,</span>
            <span class="n">providers</span> <span class="o">=</span> <span class="p">[</span><span class="n">java_common</span><span class="p">.</span><span class="n">JavaRuntimeInfo</span><span class="p">],</span>
        <span class="p">),</span>
    <span class="p">},</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">{</span><span class="s">"index"</span><span class="p">:</span> <span class="s">"%{name}_index.jar"</span><span class="p">},</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Further improvements can be made, to give this index-like support to the Java compiler itself and not only for <code class="language-plaintext highlighter-rouge">java_binary</code> targets.</p>

<p>We’ve gone out of our way on these systems to define our inputs, enforce contraints and model our dependencies. Not taking advantage of
this stability and regressing to the default search often found in our tooling is leaving easy performance improvements on the floor.</p>
              </div>
            </article>
        </main>

      <aside>
        <img src="logo.svg">
        <p>Last updated: 2025-01-12 11:17</p>
        <ul>
          
            <li>
              <a href="https://marcus.means.no/https://marcus.means.no/">
                @’s marcus - marcus
                </a>
            </li>
          
            <li>
              <a href="https://tazjin/feed.atom">
                tazjin's interblag
                </a>
            </li>
          
            <li>
              <a href="https://blog.ericv.me/atom.xml">
                Eric V's Blog
                </a>
            </li>
          
            <li>
              <a href="https://fzakaria.com/feed.xml">
                Farid Zakaria’s Blog
                </a>
            </li>
          
            <li>
              <a href="https://tech.j4m3s.eu/">
                j4m3s' tech blog
                </a>
            </li>
          
            <li>
              <a href="https://ghuntley.com/">
                Geoffrey Huntley
                </a>
            </li>
          
            <li>
              <a href="https://flokli.de/posts/">
                Posts on flokli
                </a>
            </li>
          
            <li>
              <a href="https://elis.nu/blog/">
                ~elis/blog/ on Elis Hirwing
                </a>
            </li>
          
            <li>
              <a href="https://andreas.rammhold.de/">
                Andreas Rammhold
                </a>
            </li>
          
            <li>
              <a href="https://leahneukirchen.org/blog">
                leah blogs
                </a>
            </li>
          
            <li>
              <a href="https://blog.benjojo.co.uk/">
                benjojo blog
                </a>
            </li>
          
            <li>
              <a href="https://eta.st/">
                eta.st
                </a>
            </li>
          
            <li>
              <a href="https://tvl.fyi/feed.atom">
                TVL blog
                </a>
            </li>
          
        </ul>
      </aside>
    </div>
  </body>
</html>
